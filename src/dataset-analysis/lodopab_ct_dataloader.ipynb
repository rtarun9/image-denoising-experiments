{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'set' has no attribute '__all__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path, PathLike\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdival\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfbp_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cached_fbp_dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdival\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomAccessTorchDataset\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32mc:\\Users\\Tarun\\miniconda3\\lib\\site-packages\\dival\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG, get_config, set_config\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPairs\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstandard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_standard_dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreference_reconstructors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_reference_reconstructor\n",
      "File \u001b[1;32mc:\\Users\\Tarun\\miniconda3\\lib\\site-packages\\dival\\datasets\\__init__.py:34\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     29\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_standard_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGroundTruthDataset\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservationGroundTruthPairDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEllipsesDataset\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoDoPaBDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAngleSubsetDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCachedDataset\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerate_cache_files\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFBPDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReorderedDataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstandard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_standard_dataset\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Dataset, GroundTruthDataset,\n\u001b[0;32m     36\u001b[0m                       ObservationGroundTruthPairDataset)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mellipses_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EllipsesDataset\n",
      "File \u001b[1;32mc:\\Users\\Tarun\\miniconda3\\lib\\site-packages\\dival\\datasets\\standard.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01modl\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdival\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mellipses_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EllipsesDataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdival\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlodopab_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoDoPaBDataset\n",
      "File \u001b[1;32mc:\\Users\\Tarun\\miniconda3\\lib\\site-packages\\odl\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Propagate names defined in` __all__` of all \"core\" subpackages into\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# the top-level namespace\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 48\u001b[0m __all__ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__all__\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     51\u001b[0m __all__ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m space\u001b[38;5;241m.\u001b[39m__all__\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'set' has no attribute '__all__'"
     ]
    }
   ],
   "source": [
    "# From : https://raw.githubusercontent.com/liutianlin0121/musc/main/musc/dataloaders/load_lodopab_ct.py\n",
    "\"\"\"Dataloaders for lodopab CT dataset\"\"\"\n",
    "from os import path, PathLike\n",
    "import typing\n",
    "from dival.datasets.fbp_dataset import get_cached_fbp_dataset\n",
    "from dival.util.torch_utility import RandomAccessTorchDataset\n",
    "from torch.utils.data import DataLoader\n",
    "clean_file_name = '../../../../Dataset/LoDoPaB-CT/ground_truth_validation/ground_truth_validation_000.hdf5'\n",
    "DATASET_DIR = '../../../../Dataset/LoDoPaB-CT/ground_truth_validation/'\n",
    "\n",
    "\n",
    "def get_dataloaders_ct(batch_size: int = 1,\n",
    "                       num_workers: int = 0,\n",
    "                       cache_dir: typing.Union[str, PathLike] = path.join(\n",
    "                           DATASET_DIR),\n",
    "                       include_validation: bool = True,\n",
    "                       train_percent: int = 100,\n",
    "                       validation_len: int = 100,\n",
    "                       test_len: int = 100\n",
    "                       ):\n",
    "    \"\"\"Construct pytorch loader for the LoDoPaB CT dataset.\n",
    "\n",
    "    This function follows the logic here:\n",
    "    https://github.com/jleuschn/dival/blob/master/dival/examples/ct_train_fbpunet.py#L25-L43\n",
    "\n",
    "    Args:\n",
    "        batch_size (int, optional): batch size for training.\n",
    "            Defaults to 1.\n",
    "        num_workers (int, optional): Defaults to 0.\n",
    "        cache_dir (_type_, optional): Defaults to\n",
    "            path.join(DATASET_DIR, 'cache_lodopab').\n",
    "        include_validation (bool, optional): Defaults to True.\n",
    "        train_percent (int, optional): Defaults to 100.\n",
    "        validation_len (int, optional): Defaults to 100.\n",
    "        test_len (int, optional): Defaults to 100.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: dataloaders for training, validation, and test.\n",
    "    \"\"\"\n",
    "    if include_validation:\n",
    "        parts = ['train', 'validation', 'test']\n",
    "        batch_sizes = {'train': batch_size, 'validation': 1, 'test': 1}\n",
    "\n",
    "    else:\n",
    "        parts = ['train', 'test']\n",
    "        batch_sizes = {'train': batch_size, 'test': 1}\n",
    "\n",
    "    cache_files = {\n",
    "        part: (path.join(cache_dir,\n",
    "                         'cache_lodopab_' + part + '_fbp.npy'), None)\n",
    "        for part in parts\n",
    "    }\n",
    "\n",
    "    standard_dataset = get_standard_dataset('lodopab', impl='astra_cuda')\n",
    "    ray_trafo = standard_dataset.get_ray_trafo(impl='astra_cuda')\n",
    "    dataset = get_cached_fbp_dataset(standard_dataset, ray_trafo, cache_files)\n",
    "\n",
    "    dataset.train_len = int(dataset.train_len * train_percent / 100)\n",
    "\n",
    "    print('train percent: ', train_percent)\n",
    "    print('train dataset len: ', dataset.train_len)\n",
    "    dataset.validation_len = validation_len\n",
    "    dataset.test_len = test_len\n",
    "\n",
    "    # create PyTorch datasets\n",
    "    datasets = {\n",
    "        x: RandomAccessTorchDataset(dataset=dataset,\n",
    "                                    part=x,\n",
    "                                    reshape=((1, ) + dataset.space[0].shape,\n",
    "                                             (1, ) + dataset.space[1].shape))\n",
    "        for x in parts\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(datasets[x],\n",
    "                      batch_size=batch_sizes[x],\n",
    "                      pin_memory=True,\n",
    "                      shuffle=(x == 'train'),\n",
    "                      num_workers=num_workers)\n",
    "        for x in parts\n",
    "    }\n",
    "\n",
    "    return dataloaders\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
