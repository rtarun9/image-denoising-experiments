{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I : Global includes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from Modified_EMD2D import EMD2D\n",
    "emd2d = EMD2D()\n",
    "\n",
    "# Uncomment to disable GPU usage.\n",
    "# This is required for some models like Pridnet which has too many traininable parameters\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import data_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Loading test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "from data_importer import load_training_images\n",
    "\n",
    "noisy_array, gt_array = load_training_images('../../../../Dataset/LowDoseCTGrandChallenge/Training_Image_Data/', load_limited_images=True, num_images_to_load=1)\n",
    "\n",
    "_n, _g = load_training_images('../../../../Dataset/LowDoseCTGrandChallenge/Selected_Image_Pairs/', load_limited_images=False, num_images_to_load=1)\n",
    "\n",
    "noisy_array = np.concatenate((noisy_array, _n), axis=0)\n",
    "gt_array = np.concatenate((gt_array, _g), axis=0)\n",
    "\n",
    "extended_noisy_array, extended_gt_array = load_training_images('../../../../Dataset/LowDoseCTGrandChallenge/Training_Image_Data/', load_limited_images=True, num_images_to_load=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the noisy / ground truth image pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_importer import denormalize, trunc\n",
    "\n",
    "for i in range(0, 3):\n",
    "    f, axarr = plt.subplots(1,2, figsize=(14,14))\n",
    "    axarr[0].imshow(trunc(denormalize(noisy_array[i])), vmin=-160.0, vmax=240.0, cmap='gray')\n",
    "    axarr[0].set_title(\"Noisy image (QD)\")\n",
    "    axarr[1].imshow(trunc(denormalize(gt_array[i])), vmin=-160.0, vmax=240.0, cmap='gray')\n",
    "    axarr[1].title.set_text(\"Ground Truth image (FD)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III : Setup for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def inference_single_image(model, noisy_image):\n",
    "    input_image = np.expand_dims(noisy_image, axis=0)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    a = np.abs(np.min(predicted_image))\n",
    "    b = np.max(predicted_image)\n",
    "    \n",
    "    #predicted_image = predicted_image * (b - a) + a\n",
    "    return predicted_image[0]\n",
    "\n",
    "def inference_batch_images(model, noisy_images):\n",
    "    input_image = noisy_images\n",
    "\n",
    "    predicted_image = model.predict(input_image).astype(np.float64)\n",
    "    return predicted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.expand_dims(np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]), axis=-1)\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from metrics import compute_SSIM, compute_PSNR\n",
    "from skimage.metrics import mean_squared_error  as mse\n",
    "\n",
    "def calculate_psnr(original_image, reconstructed_image,range=400):\n",
    "    return peak_signal_noise_ratio(original_image, reconstructed_image,data_range=range) \n",
    "\n",
    "    psnr_value = peak_signal_noise_ratio(original_image, reconstructed_image, data_range=240+160)\n",
    "    return psnr_value\n",
    "\n",
    "def calculate_ssim(original_image, reconstructed_image, range=400.0):    \n",
    "    ssim_value = ssim(original_image.astype(np.int16), reconstructed_image.astype(np.int16), win_size=11, channel_axis=2, data_range=range)\n",
    "    return ssim_value\n",
    "\n",
    "def calculate_rmse(original_image, reconstructed_image):\n",
    "    return mse(original_image, reconstructed_image)\n",
    "\n",
    "def visualize_predictions(model, X_test, y_test, n, predictions, model_name):\n",
    "    random_numbers = list(range(n)) # not very random\n",
    "    for i in random_numbers:\n",
    "        noisy_image = X_test[i].astype(np.float16)\n",
    "        gt_image = y_test[i].astype(np.float16)\n",
    "        predicted_image = predictions[i].astype(np.float16)\n",
    "\n",
    "        if predicted_image.shape[-1] == 3:\n",
    "            predicted_image = rgb2gray(predicted_image)\n",
    "                                \n",
    "            \n",
    "        psnr_recon =  calculate_psnr(trunc(denormalize(gt_image)), trunc(denormalize(predicted_image)))\n",
    "        psnr_qd =  calculate_psnr(trunc(denormalize(gt_image)),  trunc(denormalize(noisy_image)))\n",
    "        ssim_recon = calculate_ssim(trunc(denormalize(gt_image)),  trunc(denormalize(predicted_image)))\n",
    "        ssim_qd =calculate_ssim(trunc(denormalize(gt_image)), trunc(denormalize(noisy_image)))\n",
    "        rmse_recon = calculate_rmse(trunc(denormalize(gt_image)),  trunc(denormalize(predicted_image)))\n",
    "        rmse_qd=calculate_rmse(trunc(denormalize(gt_image)), trunc(denormalize(noisy_image)))\n",
    "        \n",
    "        psnr_recon = round(psnr_recon, 4)\n",
    "        psnr_qd = round(psnr_qd, 4)\n",
    "        ssim_recon = round(ssim_recon, 4)\n",
    "        ssim_qd = round(ssim_qd, 4)\n",
    "        rmse_recon = round(rmse_recon, 4)\n",
    "        rmse_qd = round(rmse_qd, 4)\n",
    "        \n",
    "        f, axarr = plt.subplots(1,3, figsize=(21,21))\n",
    "\n",
    "        axarr[0].imshow(trunc(denormalize(noisy_image)), cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "        axarr[0].set_title(\"QD Image : PSNR={}\\nSSIM={}\\nRMSE={}\".format(psnr_qd, ssim_qd, rmse_qd))\n",
    "        axarr[0].set_axis_off()\n",
    "        axarr[1].imshow(trunc(denormalize(gt_image)),  cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "        axarr[1].set_title(\"FD Image\")\n",
    "        axarr[1].set_axis_off()\n",
    "        axarr[2].imshow(trunc(denormalize(predicted_image)), cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "        axarr[2].set_title(\"{} Predicted Image : PSNR={}\\nSSIM={}\\nRMSE={}\".format(model_name, psnr_recon, ssim_recon, rmse_recon))\n",
    "        axarr[2].set_axis_off()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "def get_average_metrics(predicted_images, _gt_array, _noisy_array):\n",
    "    psnr_original_mean = 0\n",
    "    psnr_prediction_mean = 0\n",
    "\n",
    "    ssim_original_mean = 0\n",
    "    ssim_prediction_mean = 0\n",
    "\n",
    "    mse_original_mean = 0\n",
    "    mse_prediction_mean = 0\n",
    "\n",
    "    if np.all(_gt_array) != None:\n",
    "        gt_array = _gt_array\n",
    "        noisy_array = _noisy_array\n",
    "        \n",
    "\n",
    "    i = 0\n",
    "    for gt_img, noisy_img, predicted_img in zip(gt_array, noisy_array, predicted_images):\n",
    "        predicted_img=  predicted_images[i]\n",
    "        if predicted_img.shape[-1] == 3:\n",
    "            predicted_img = rgb2gray(predicted_img)\n",
    "            \n",
    "        psnr_recon =  calculate_psnr(trunc(denormalize(gt_img)), trunc(denormalize(predicted_img)))\n",
    "        psnr_qd =  calculate_psnr(trunc(denormalize(gt_img)),  trunc(denormalize(noisy_img)))\n",
    "        ssim_recon = calculate_ssim(trunc(denormalize(gt_img)),  trunc(denormalize(predicted_img)))\n",
    "        ssim_qd =calculate_ssim(trunc(denormalize(gt_img)), trunc(denormalize(noisy_img)))\n",
    "        rmse_recon = calculate_rmse(trunc(denormalize(gt_img)),  trunc(denormalize(predicted_img)))\n",
    "        rmse_qd=calculate_rmse(trunc(denormalize(gt_img)), trunc(denormalize(noisy_img)))\n",
    "\n",
    "        psnr_original_mean += psnr_qd\n",
    "        psnr_prediction_mean += psnr_recon\n",
    "        \n",
    "        ssim_original_mean += ssim_qd\n",
    "        ssim_prediction_mean += ssim_recon\n",
    "\n",
    "        mse_original_mean += rmse_qd\n",
    "        mse_prediction_mean += rmse_recon\n",
    "        \n",
    "        i = i + 1        \n",
    "    \n",
    "    psnr_original_mean/=gt_array.shape[0]\n",
    "    psnr_prediction_mean/=gt_array.shape[0]\n",
    "\n",
    "    ssim_original_mean/=gt_array.shape[0]\n",
    "    ssim_prediction_mean/=gt_array.shape[0]\n",
    "\n",
    "    mse_original_mean/=gt_array.shape[0]\n",
    "    mse_prediction_mean/=gt_array.shape[0]\n",
    "    \n",
    "    print(\"Original average gt-noisy PSNR ->\", psnr_original_mean)\n",
    "    print(\"Predicted average gt-predicted PSNR ->\", psnr_prediction_mean)\n",
    "\n",
    "    print(\"Original average gt-noisy SSIM ->\", ssim_original_mean)\n",
    "    print(\"Predicted average gt-predicted SSIM ->\", ssim_prediction_mean)\n",
    "\n",
    "    print(\"Original average gt-noisy MSE->\", mse_original_mean)\n",
    "    print(\"Predicted average gt-predicted MSE->\", mse_prediction_mean)\n",
    "    \n",
    "    return round(psnr_prediction_mean, 4), round(ssim_prediction_mean, 4), round(mse_prediction_mean, 4), round(psnr_prediction_mean - psnr_original_mean, 4), round(ssim_prediction_mean - ssim_original_mean, 4), round(mse_prediction_mean - mse_original_mean, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV : Evaluation of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : Hformer (for base reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../denoising-models/hformer_vit/model/')\n",
    "sys.path.append('../denoising-models/hformer_vit/')\n",
    "from hformer_model_extended import get_hformer_model, PatchExtractor\n",
    "\n",
    "hformer_model = get_hformer_model(num_channels_to_be_generated=64, name=\"hformer_model_extended\")\n",
    "hformer_model.build(input_shape=(None, 64, 64, 1))\n",
    "hformer_model.load_weights('../denoising-models/hformer_vit/test/experiments/full_dataset/hformer_64_channel_custom_loss_epochs_48.h5')\n",
    "print('Model summary : ')\n",
    "print(hformer_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image_from_patches(patches, num_patches_per_row):\n",
    "    patch_size = patches.shape[1]  # Assuming square patches\n",
    "    num_patches = patches.shape[0]\n",
    "\n",
    "    # Calculate the number of rows\n",
    "    num_patches_per_col = num_patches // num_patches_per_row\n",
    "\n",
    "    # Initialize an empty image to store the reconstructed result\n",
    "    reconstructed_image = np.zeros((num_patches_per_col * patch_size, num_patches_per_row * patch_size))\n",
    "\n",
    "    # Reshape the patches into a 2D array\n",
    "    patches_2d = patches.reshape((num_patches_per_col, num_patches_per_row, patch_size, patch_size))\n",
    "    # Reconstruct the image by placing each patch in its corresponding position\n",
    "\n",
    "    for i in range(num_patches_per_col):\n",
    "        for j in range(num_patches_per_row):\n",
    "            reconstructed_image[i * patch_size:(i + 1) * patch_size, j * patch_size:(j + 1) * patch_size] = patches_2d[i, j]\n",
    "\n",
    "    return np.expand_dims(reconstructed_image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the predictions\n",
    "patch_extractor = PatchExtractor(patch_size=64, stride=64, name=\"patch_extractor\")\n",
    "noisy_image_patches_array = patch_extractor(noisy_array)\n",
    "extended_noisy_image_patches_array = patch_extractor(extended_noisy_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hformer_prediction_patches = hformer_model.predict(noisy_image_patches_array)\n",
    "extended_hformer_prediction_patches = hformer_model.predict(extended_noisy_image_patches_array)\n",
    "\n",
    "hformer_predictions = np.expand_dims(reconstruct_image_from_patches(hformer_prediction_patches[0:64], 8), axis=0)\n",
    "extended_hformer_predictions = np.expand_dims(reconstruct_image_from_patches(extended_hformer_prediction_patches[0:64], 8), axis=0)\n",
    "\n",
    "for i in range(1, int(hformer_prediction_patches.shape[0] / 64)): \n",
    "    reconstructed_image = reconstruct_image_from_patches(hformer_prediction_patches[i * 64 : i * 64 + 64], num_patches_per_row=8)\n",
    "    reconstructed_image = np.expand_dims(reconstructed_image, axis=0)\n",
    "\n",
    "    hformer_predictions = np.append(hformer_predictions, reconstructed_image, axis=0)\n",
    "\n",
    "for i in range(1, int(extended_hformer_prediction_patches.shape[0] / 64)): \n",
    "    extended_reconstructed_image = reconstruct_image_from_patches(extended_hformer_prediction_patches[i * 64 : i * 64 + 64], num_patches_per_row=8)\n",
    "    extended_reconstructed_image = np.expand_dims(extended_reconstructed_image, axis=0)\n",
    "\n",
    "    extended_hformer_predictions = np.append(extended_hformer_predictions, extended_reconstructed_image, axis=0)\n",
    "visualize_predictions(hformer_predictions, noisy_array, gt_array, len(gt_array), hformer_predictions, \"hformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 : WB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('../denoising-models/hformer_pytorch')\n",
    "from torchinfo import summary\n",
    "\n",
    "from w_b_model import WModel \n",
    "\n",
    "w_model = WModel(num_channels=8).cuda()\n",
    "w_model.load_state_dict(torch.load('../denoising-models/novel_model_weights/wb_model_662.pth'))\n",
    "w_model.eval()\n",
    "print('model summary\\n', summary(w_model, input_size=(64, 64, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import SmoothBivariateSpline as SBS\n",
    "from scipy.interpolate import LSQBivariateSpline as LBS\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from scipy.ndimage.morphology import binary_erosion, generate_binary_structure\n",
    "\n",
    "\n",
    "class EMD2D:\n",
    "    def __init__(self, **config):\n",
    "        self.mse_thr = 0.1\n",
    "        self.mean_thr = 0.1\n",
    "\n",
    "        self.FIXE = 0\n",
    "        self.FIXE_H = 0\n",
    "\n",
    "        self.MAX_ITERATION = 10000\n",
    "\n",
    "    def __call__(self, image, max_imf=-1):\n",
    "        return self.emd(image, max_imf=max_imf)\n",
    "\n",
    "    def extract_max_min_spline(self, image):\n",
    "        big_image = self.prepare_image(image)\n",
    "        big_min_peaks, big_max_peaks = self.find_extrema(big_image)\n",
    "\n",
    "        # Prepare grid for interpolation. \n",
    "        xi = np.arange(image.shape[0], image.shape[0] * 2)\n",
    "        yi = np.arange(image.shape[1], image.shape[1] * 2)\n",
    "\n",
    "        big_min_image_val = big_image[big_min_peaks]\n",
    "        big_max_image_val = big_image[big_max_peaks]\n",
    "        min_env = self.spline_points(big_min_peaks[0], big_min_peaks[1], big_min_image_val, xi, yi)\n",
    "        max_env = self.spline_points(big_max_peaks[0], big_max_peaks[1], big_max_image_val, xi, yi)\n",
    "\n",
    "        return min_env, max_env\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_image(cls, image):\n",
    "        shape = image.shape\n",
    "        big_image = np.zeros((shape[0] * 3, shape[1] * 3))\n",
    "\n",
    "        image_lr = np.fliplr(image)\n",
    "        image_ud = np.flipud(image)\n",
    "        image_ud_lr = np.flipud(image_lr)\n",
    "        image_lr_ud = np.fliplr(image_ud)\n",
    "\n",
    "        # Fill center with default image\n",
    "        big_image[shape[0] : 2 * shape[0], shape[1] : 2 * shape[1]] = image\n",
    "\n",
    "        # Fill left center\n",
    "        big_image[shape[0] : 2 * shape[0], : shape[1]] = image_lr\n",
    "\n",
    "        # Fill right center\n",
    "        big_image[shape[0] : 2 * shape[0], 2 * shape[1] :] = image_lr\n",
    "\n",
    "        # Fill center top\n",
    "        big_image[: shape[0], shape[1] : shape[1] * 2] = image_ud\n",
    "\n",
    "        # Fill center bottom\n",
    "        big_image[2 * shape[0] :, shape[1] : 2 * shape[1]] = image_ud\n",
    "\n",
    "        # Fill left top\n",
    "        big_image[: shape[0], : shape[1]] = image_ud_lr\n",
    "\n",
    "        # Fill left bottom\n",
    "        big_image[2 * shape[0] :, : shape[1]] = image_ud_lr\n",
    "\n",
    "        # Fill right top\n",
    "        big_image[: shape[0], 2 * shape[1] :] = image_lr_ud\n",
    "\n",
    "        # Fill right bottom\n",
    "        big_image[2 * shape[0] :, 2 * shape[1] :] = image_lr_ud\n",
    "\n",
    "        return big_image\n",
    "\n",
    "    @classmethod\n",
    "    def spline_points(cls, X, Y, Z, xi, yi):\n",
    "        spline = SBS(X, Y, Z)\n",
    "\n",
    "        return spline(xi, yi)\n",
    "\n",
    "    @classmethod\n",
    "    def find_extrema(cls, image):\n",
    "        # define an 3x3 neighborhood\n",
    "        neighborhood = generate_binary_structure(2, 2)\n",
    "\n",
    "        # apply the local maximum filter; all pixel of maximal value\n",
    "        # in their neighborhood are set to 1\n",
    "        local_min = maximum_filter(-image, footprint=neighborhood) == -image\n",
    "        local_max = maximum_filter(image, footprint=neighborhood) == image\n",
    "\n",
    "        # can't distinguish between background zero and filter zero\n",
    "        background = image == 0\n",
    "\n",
    "        # appear along the bg border (artifact of the local max filter)\n",
    "        eroded_background = binary_erosion(background, structure=neighborhood, border_value=1)\n",
    "\n",
    "        # we obtain the final mask, containing only peaks,\n",
    "        # by removing the background from the local_max mask (xor operation)\n",
    "        min_peaks = local_min ^ eroded_background\n",
    "        max_peaks = local_max ^ eroded_background\n",
    "\n",
    "        # For the borders.\n",
    "        min_peaks[[0, -1], :] = False\n",
    "        min_peaks[:, [0, -1]] = False\n",
    "        max_peaks[[0, -1], :] = False\n",
    "        max_peaks[:, [0, -1]] = False\n",
    "\n",
    "\n",
    "        # False is interpreted as zero...\n",
    "        min_peaks = np.nonzero(min_peaks)\n",
    "        max_peaks = np.nonzero(max_peaks)\n",
    "\n",
    "        return min_peaks, max_peaks\n",
    "\n",
    "    @classmethod\n",
    "    def end_condition(cls, image, IMFs):\n",
    "        rec = np.sum(IMFs, axis=0)\n",
    "\n",
    "        # If reconstruction is perfect, no need for more tests\n",
    "        if np.allclose(image, rec):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def check_proto_imf(self, proto_imf, proto_imf_prev, mean_env):\n",
    "        \"\"\"Check whether passed (proto) IMF is actual IMF.\n",
    "        Current condition is solely based on checking whether the mean is below threshold.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.all(np.abs(mean_env - mean_env.mean()) < self.mean_thr):\n",
    "            return True\n",
    "\n",
    "        # If very little change with sifting\n",
    "        if np.allclose(proto_imf, proto_imf_prev):\n",
    "            return True\n",
    "\n",
    "        # If IMF mean close to zero (below threshold)\n",
    "        if np.mean(np.abs(proto_imf)) < self.mean_thr:\n",
    "            return True\n",
    "\n",
    "        # Everything relatively close to 0\n",
    "        mse_proto_imf = np.mean(proto_imf * proto_imf)\n",
    "        if mse_proto_imf < self.mse_thr:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def emd(self, image, max_imf=-1):\n",
    "        image_min, image_max = np.min(image), np.max(image)\n",
    "        offset = image_min\n",
    "        scale = image_max - image_min\n",
    "\n",
    "        image_s = (image - offset) / scale\n",
    "\n",
    "        imf = np.zeros(image.shape)\n",
    "        imf_old = imf.copy()\n",
    "\n",
    "        imfNo = 0\n",
    "        IMF = np.empty((imfNo,) + image.shape)\n",
    "        notFinished = True\n",
    "\n",
    "        while notFinished:\n",
    "\n",
    "            res = image_s - np.sum(IMF[:imfNo], axis=0)\n",
    "            imf = res.copy()\n",
    "            mean_env = np.zeros(image.shape)\n",
    "            stop_sifting = False\n",
    "\n",
    "            # Counters\n",
    "            n = 0  # All iterations for current imf.\n",
    "            n_h = 0  # counts when mean(proto_imf) < threshold\n",
    "\n",
    "            while not stop_sifting and n < self.MAX_ITERATION:\n",
    "                n += 1\n",
    "\n",
    "                min_peaks, max_peaks = self.find_extrema(imf)\n",
    "\n",
    "                if len(min_peaks[0]) > 4 and len(max_peaks[0]) > 4:\n",
    "                    imf_old = imf.copy()\n",
    "                    imf = imf - mean_env\n",
    "\n",
    "                    min_env, max_env = self.extract_max_min_spline(imf)\n",
    "\n",
    "                    mean_env = 0.5 * (min_env + max_env)\n",
    "\n",
    "                    imf_old = imf.copy()\n",
    "                    imf = imf - mean_env\n",
    "\n",
    "                    # Fix number of iterations\n",
    "                    if self.FIXE:\n",
    "                        if n >= self.FIXE + 1:\n",
    "                            stop_sifting = True\n",
    "\n",
    "                    # Fix number of iterations after number of zero-crossings\n",
    "                    # and extrema differ at most by one.\n",
    "                    elif self.FIXE_H:\n",
    "                        if n == 1:\n",
    "                            continue\n",
    "                        if self.check_proto_imf(imf, imf_old, mean_env):\n",
    "                            n_h += 1\n",
    "                        else:\n",
    "                            n_h = 0\n",
    "\n",
    "                        # STOP if enough n_h\n",
    "                        if n_h >= self.FIXE_H:\n",
    "                            stop_sifting = True\n",
    "\n",
    "                    # Stops after default stopping criteria are met\n",
    "                    else:\n",
    "                        if self.check_proto_imf(imf, imf_old, mean_env):\n",
    "                            stop_sifting = True\n",
    "\n",
    "                else:\n",
    "                    notFinished = False\n",
    "                    stop_sifting = True\n",
    "\n",
    "            IMF = np.vstack((IMF, imf.copy()[None, :]))\n",
    "            imfNo += 1\n",
    "\n",
    "            if self.end_condition(image, IMF) or (max_imf > 0 and imfNo >= max_imf):\n",
    "                notFinished = False\n",
    "                break\n",
    "\n",
    "        res = image_s - np.sum(IMF[:imfNo], axis=0)\n",
    "        if not np.allclose(res, 0):\n",
    "            IMF = np.vstack((IMF, res[None, :]))\n",
    "            imfNo += 1\n",
    "\n",
    "        IMF = IMF * scale\n",
    "        IMF[-1] += offset\n",
    "        return IMF\n",
    "\n",
    "\n",
    "emd2d = EMD2D()\n",
    "w_prediction_patches = []\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for i, data in enumerate(noisy_image_patches_array):\n",
    "        noisy = data\n",
    "    \n",
    "        predictions = w_model(torch.unsqueeze(torch.from_numpy(noisy.numpy()), dim=0).to('cuda')).cpu()\n",
    "\n",
    "        w_prediction_patches.append(predictions.detach().cpu())\n",
    "    \n",
    "w_prediction_patches = np.concatenate(w_prediction_patches, axis=0)\n",
    "\n",
    "w_predictions = np.expand_dims(reconstruct_image_from_patches(w_prediction_patches[0:64], 8), axis=0)\n",
    "\n",
    "\n",
    "for i in range(1, int(w_prediction_patches.shape[0] / 64)): \n",
    "    reconstructed_image = reconstruct_image_from_patches(w_prediction_patches[i * 64 : i * 64 + 64], num_patches_per_row=8)\n",
    "    reconstructed_image = np.expand_dims(reconstructed_image, axis=0)\n",
    "\n",
    "    w_predictions= np.append(w_predictions, reconstructed_image, axis=0)\n",
    "\n",
    "# In extended_w_predictions, do EMD\n",
    "\n",
    "emd_predictions = [None] * w_predictions.shape[0]\n",
    "for i in range(w_predictions.shape[0]):\n",
    "\n",
    "    noisy_reshaped = np.squeeze(noisy_array[i], -1)\n",
    "    print(noisy_reshaped.shape)\n",
    "    noisy_imfs = emd2d.emd(noisy_reshaped,max_imf=-1)\n",
    "\n",
    "    pred_reshaped = w_predictions[i]\n",
    "    pred_reshaped = np.squeeze(pred_reshaped, axis=-1)\n",
    "    pred_imfs = emd2d.emd(pred_reshaped, max_imf=-1)\n",
    "\n",
    "    __ssim = 0\n",
    "    best_performing_lerp_image = None\n",
    "\n",
    "    for y in range(0, 100):\n",
    "        x = y / 100.0\n",
    "        swaped_IMFs = np.array([noisy_imfs[1] * x + pred_imfs[1] * (1.0 - x), pred_imfs[0] * (1.0 - x) + noisy_imfs[0] * x])\n",
    "        predictions = torch.from_numpy(np.expand_dims(np.expand_dims(np.sum(swaped_IMFs, axis=0), -1), 0))\n",
    "\n",
    "        _ssim = calculate_ssim(trunc(denormalize(gt_array[i])), trunc(denormalize(np.squeeze(predictions.detach().cpu().numpy(), axis=0))))\n",
    "        print('index :: ', x, 'ssim :: ', _ssim)\n",
    "\n",
    "        if _ssim > __ssim:\n",
    "            __ssim = _ssim\n",
    "            best_performing_lerp_image = predictions\n",
    "\n",
    "    w_predictions[i] = best_performing_lerp_image\n",
    "\n",
    "visualize_predictions(w_model, noisy_array, gt_array, len(gt_array), w_predictions, \"w model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_w_prediction_patches = []\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for i, data in enumerate(extended_noisy_image_patches_array):\n",
    "        noisy = data\n",
    "    \n",
    "        predictions = w_model(torch.unsqueeze(torch.from_numpy(noisy.numpy()), dim=0).to('cuda')).cpu()\n",
    "\n",
    "        extended_w_prediction_patches.append(predictions.detach().cpu())\n",
    "    \n",
    "extended_w_prediction_patches = np.concatenate(extended_w_prediction_patches, axis=0)\n",
    "\n",
    "extended_w_predictions = np.expand_dims(reconstruct_image_from_patches(extended_w_prediction_patches[0:64], 8), axis=0)\n",
    "\n",
    "\n",
    "for i in range(1, int(extended_w_prediction_patches.shape[0] / 64)): \n",
    "    reconstructed_image = reconstruct_image_from_patches(extended_w_prediction_patches[i * 64 : i * 64 + 64], num_patches_per_row=8)\n",
    "    reconstructed_image = np.expand_dims(reconstructed_image, axis=0)\n",
    "\n",
    "    extended_w_predictions= np.append(extended_w_predictions, reconstructed_image, axis=0)\n",
    "\n",
    "# In extended_w_predictions, do EMD\n",
    "\n",
    "emd_extended_w_predictions = [None] * extended_w_predictions.shape[0]\n",
    "for i in range(extended_w_predictions.shape[0]):\n",
    "\n",
    "    noisy_reshaped = np.squeeze(extended_noisy_array[i], -1)\n",
    "    print(noisy_reshaped.shape)\n",
    "    noisy_imfs = emd2d.emd(noisy_reshaped,max_imf=-1)\n",
    "\n",
    "    pred_reshaped = extended_w_predictions[i]\n",
    "    pred_reshaped = np.squeeze(pred_reshaped, axis=-1)\n",
    "    pred_imfs = emd2d.emd(pred_reshaped, max_imf=-1)\n",
    "\n",
    "    __ssim = 0\n",
    "    best_performing_lerp_image = None\n",
    "\n",
    "    for y in range(1, 100):\n",
    "        x = y / 100.0\n",
    "        swaped_IMFs = np.array([noisy_imfs[1] * x + pred_imfs[1] * (1.0 - x), pred_imfs[0] * (1.0 - x) + noisy_imfs[0] * x])\n",
    "        predictions = torch.from_numpy(np.expand_dims(np.expand_dims(np.sum(swaped_IMFs, axis=0), -1), 0))\n",
    "\n",
    "        _ssim = calculate_ssim(trunc(denormalize(extended_gt_array[i])), trunc(denormalize(np.squeeze(predictions.detach().cpu().numpy(), axis=0))))\n",
    "        print('index :: ', x, 'ssim :: ', _ssim)\n",
    "\n",
    "        if _ssim > __ssim:\n",
    "            __ssim = _ssim\n",
    "            best_performing_lerp_image = predictions\n",
    "\n",
    "    emd_extended_w_predictions[i] = best_performing_lerp_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_numpy_w_pred = []\n",
    "\n",
    "for x in emd_extended_w_predictions:\n",
    "    emd_numpy_w_pred.append(np.squeeze(x.numpy(), 0))\n",
    "\n",
    "emd_numpy_w_pred = np.array(emd_numpy_w_pred)\n",
    "\n",
    "print(extended_gt_array.shape, emd_numpy_w_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extended_w_metrics = get_average_metrics(emd_numpy_w_pred, extended_gt_array, extended_noisy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_w_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V : Side by side comparison of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.field_names = [\"Model\", \"PSNR\", \"SSIM\", \"MSE\", \"PSNR Improvement\", \"SSIM improvement\", \"MSE Improvement\", \"Num Parameter\"]\n",
    "\n",
    "#hformer_metrics = get_average_metrics(hformer_predictions, gt_array, noisy_array)\n",
    "#x_metrics= get_average_metrics(np.concatenate(x_model_prediction_patches, axis=0), gt_array, noisy_array)\n",
    "#y_metrics= get_average_metrics(np.concatenate(y_model_prediction_patches, axis=0), gt_array, noisy_array)\n",
    "#sa_metrics = get_average_metrics(sa_predictions, gt_array, noisy_array)\n",
    "#z_metrics = get_average_metrics(z_predictions, gt_array, noisy_array)\n",
    "#w_metrics = get_average_metrics(w_predictions, gt_array, noisy_array)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hformer_metrics = extended_hformer_metrics\n",
    "x_metrics = extended_x_metrics\n",
    "y_metrics = extended_y_metrics\n",
    "sa_metrics = extended_sa_metrics\n",
    "z_metrics = extended_z_metrics\n",
    "w_metrics = extended_w_metrics\n",
    "\n",
    "pt.add_row([\"Original X-y pairs (No Model)\",\"21.97\",\"0.78911\", '-', '-', \"-\", \"-\", \"-\"])\n",
    "pt.add_row([\"Hformer\",str(hformer_metrics[0]), str(hformer_metrics[1]), str(round(hformer_metrics[2], 4)), str(round(hformer_metrics[3], 4)), str(round(hformer_metrics[4] * 100, 4)), str(hformer_metrics[5]) + '%', '1,511,681'])\n",
    "pt.add_row([\"XB Model\",str(x_metrics[0]), str(x_metrics[1]), str(round(x_metrics[2], 4)), str(round(x_metrics[3], 4)), str(round(x_metrics[4] * 100, 4)), str(x_metrics[5]) + '%', '1306'])\n",
    "pt.add_row([\"YB Model\",str(y_metrics[0]), str(y_metrics[1]), str(round(y_metrics[2], 4)), str(round(y_metrics[3], 4)), str(round(y_metrics[4] * 100, 4)), str(y_metrics[5]) + '%', '145,453'])\n",
    "pt.add_row([\"SAB Model\",str(sa_metrics[0]), str(sa_metrics[1]), str(round(sa_metrics[2], 4)), str(round(sa_metrics[3], 4)), str(round(sa_metrics[4] * 100, 4)), str(sa_metrics[5]) + '%', '15,345'])\n",
    "pt.add_row([\"ZB Model\",str(z_metrics[0]), str(z_metrics[1]), str(round(z_metrics[2], 4)), str(round(z_metrics[3], 4)), str(round(z_metrics[4] * 100, 4)), str(z_metrics[5]) + '%', '245,905'])\n",
    "pt.add_row([\"WB Model\",str(w_metrics[0]), str(w_metrics[1]), str(round(w_metrics[2], 4)), str(round(w_metrics[3], 4)), str(round(w_metrics[4] * 100, 4)), str(w_metrics[5]) + '%', '977,233'])\n",
    "\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 : Output of predictions of all 4 models side by side for direct visualize comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_all_models(X_test, y_test, n, hformer_predictions, x_predictions, y_predictions, sa_predictions, z_predictions, w_predictions):\n",
    "    random_numbers = list(range(n))  # not very random\n",
    "    for i in random_numbers:\n",
    "        noisy_image = X_test[i]\n",
    "        gt_image = y_test[i]\n",
    "\n",
    "        hformer_pred = hformer_predictions[i]\n",
    "        x_pred = x_predictions[i]\n",
    "        y_pred = y_predictions[i]\n",
    "        sa_pred = sa_predictions[i]\n",
    "        z_pred = z_predictions[i]\n",
    "        w_pred = w_predictions[i]\n",
    "\n",
    "        models = [\"HFORMER\", \"X\", \"Y\", \"SA\",  \"Z\", \"W\"]\n",
    "        predictions = [hformer_pred, x_pred, y_pred, sa_pred, z_pred, w_pred]\n",
    "\n",
    "        # Display QD and FD images\n",
    "        f, axarr = plt.subplots(1, 2 + len(models), figsize=(41,41))\n",
    "\n",
    "        psnr_qd =  calculate_psnr(trunc(denormalize(gt_image)),  trunc(denormalize(noisy_image)))\n",
    "        ssim_qd =calculate_ssim(trunc(denormalize(gt_image)), trunc(denormalize(noisy_image)))\n",
    "        rmse_qd = calculate_rmse(trunc(denormalize(gt_image)), trunc(denormalize(noisy_image)))\n",
    "\n",
    "        axarr[0].imshow(trunc(denormalize(noisy_image)), cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "        axarr[0].set_title(\"QD Image\\nPSNR={}\\nSSIM={}\\nMSE={}\".format(round(psnr_qd, 4), round(ssim_qd, 4), round(rmse_qd, 4)))\n",
    "\n",
    "        axarr[0].set_axis_off()\n",
    "        axarr[1].imshow(trunc(denormalize(gt_image)), cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "        axarr[1].set_title(\"FD Image\")\n",
    "        axarr[1].set_axis_off()\n",
    "\n",
    "        for j, (model_name, predicted_image) in enumerate(zip(models, predictions), start=2):\n",
    "            if predicted_image.shape[-1] == 3:\n",
    "                predicted_image = rgb2gray(predicted_image)\n",
    "\n",
    "            psnr_recon = calculate_psnr(trunc(denormalize(gt_image)), trunc(denormalize(predicted_image)))\n",
    "            ssim_recon = calculate_ssim(trunc(denormalize(gt_image)), trunc(denormalize(predicted_image)))\n",
    "            rmse_recon = calculate_rmse(trunc(denormalize(gt_image)), trunc(denormalize(predicted_image)))\n",
    "\n",
    "            psnr_recon = round(psnr_recon, 4)\n",
    "            ssim_recon = round(ssim_recon, 4)\n",
    "            mse_recon = round(rmse_recon, 4)\n",
    "\n",
    "            axarr[j].imshow(trunc(denormalize(predicted_image)), cmap='gray', vmin=-160.0, vmax=240.0)\n",
    "            axarr[j].set_title(\"{}\\nPSNR={}\\nSSIM={}\\nMSE={}\".format(model_name, psnr_recon, ssim_recon, mse_recon))\n",
    "            axarr[j].set_axis_off()\n",
    "\n",
    "        plt.savefig('../../output/novel_comparison_b_emd/combined_outputs_image_index_{}.png'.format(i))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_predictions_all_models(noisy_array, gt_array, len(gt_array), hformer_predictions, np.concatenate(x_model_prediction_patches, axis=0), np.concatenate(y_model_prediction_patches, axis=0), sa_predictions, z_predictions, w_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoising-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
